{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy.io, random, csv\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General helper funcs\n",
    "\n",
    "# Usage results_to_csv(clf.predict(X_test))\n",
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1  # Ensures that the index starts at 1. \n",
    "    df.to_csv('submission.csv', index_label='Id')\n",
    "    \n",
    "# partition dataset into training and validation set based on their indices\n",
    "def sample(data, label, numSamples):\n",
    "    totalIndices = np.arange(len(data))\n",
    "    np.random.shuffle(totalIndices)\n",
    "    validIndices, trainIndices = totalIndices[:numSamples], totalIndices[numSamples:]\n",
    "    valid_data = data[validIndices]\n",
    "    valid_label = label[validIndices]\n",
    "    train_data = data[trainIndices]\n",
    "    train_label = label[trainIndices]\n",
    "    return valid_data, valid_label, train_data, train_label\n",
    "\n",
    "seed = 266\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "\n",
    "def loadData(dataset):\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data       \n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "        # Preprocess data\n",
    "#         path_train = 'datasets/titanic/titanic_training.csv'\n",
    "#         data = pd.read_csv(path_train)\n",
    "#         path_test = 'datasets/titanic/titanic_testing_data.csv'\n",
    "#         test_data = pd.read_csv(path_test)\n",
    "\n",
    "#         X, y, features = preprocessTitanic(data)\n",
    "#         Z, _, f2 = preprocessTitanic(test_data)\n",
    "\n",
    "        # Read in preprocessed data\n",
    "        path_train = 'datasets/titanic/titanic_training_processed.csv'\n",
    "        path_test = 'datasets/titanic/titanic_test_processed.csv'\n",
    "\n",
    "        train_data = pd.read_csv(path_train)\n",
    "        features = train_data.columns.values\n",
    "        y = train_data['survived'].values\n",
    "        train_data.drop(['survived'], axis=1, inplace=True)\n",
    "        X = train_data.values\n",
    "        \n",
    "        test_data = pd.read_csv(path_test)\n",
    "        Z = test_data.values\n",
    "        f2 = test_data.columns.values\n",
    "\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = 'datasets/spam-dataset/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data'] #5172 entries\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data'] #5857 entries\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "        \n",
    "    return X, y, Z, features, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for preprocessing Titanic data\n",
    "\n",
    "# Fill in NaN with constant\n",
    "def fillInWithConstant(val, constant, firstLetter=False):\n",
    "    if val != val: #nan\n",
    "        return constant\n",
    "    if firstLetter:\n",
    "        return val[0]\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "# Group tickets into categories\n",
    "def getTicketGroup(ticket):\n",
    "    if ticket != ticket:\n",
    "        return ticket\n",
    "    \n",
    "    ticketGroups = {\n",
    "        \"A4\": [\"A/4\", \"A4.\", \"A. 2.\"],\n",
    "        \"A5\": [\"A./5.\", \"A./5\", \"A/5\", \"A/5.\", \"A/S\", \"A.5\", \"AQ\"],\n",
    "        \"CA\" : [\"C.A\", \"CA\", \"C \"],\n",
    "        \"FC\": [\"F.C.\", \"Fa\"],\n",
    "        \"PC\": [\"PC\"],\n",
    "        \"PP\": [\"PP\"],\n",
    "        \"SC\": [\"S.C.\", \"SC/\", \"SCO\"],\n",
    "        \"SO\": [\"S.O.\", \"SO/\"],\n",
    "        \"SOTON\": [\"SOTON\", \"STON\"],\n",
    "        \"WC\": [\"W./C.\", \"W/C\"],\n",
    "        \"WE\": [\"W.E.P\", \"WE/P\"]\n",
    "    }\n",
    "    for group in ticketGroups.keys():\n",
    "        for name in ticketGroups[group]:\n",
    "            if name in ticket:\n",
    "                return group\n",
    "    \n",
    "    if len(ticket) > 7: #letter&number\n",
    "        print(ticket)\n",
    "        ticketParts = ticket.split(\" \")\n",
    "        ticket = ticketParts[len(ticketParts)-1]\n",
    "        if len(ticket) < 4:\n",
    "            print('todo')\n",
    "    elif len(ticket) == 7:\n",
    "        return \"SOTON\"\n",
    "    elif len(ticket) == 6:\n",
    "        return ticket[0] + \"xxxxx\"\n",
    "    elif len(ticket) == 5 and ticket[0] == '1' and ticket[1] <= '3': \n",
    "        return \"FC\"\n",
    "    elif len(ticket) == 5 and ticket[0] == '1' and ticket[1] > '3': \n",
    "        return \"PC\"\n",
    "    elif len(ticket) == 4 and ticket[0] == '2': \n",
    "        return \"SC\"\n",
    "    elif len(ticket) == 4 and ticket[0] == '6': \n",
    "        return \"WC\"\n",
    "    elif len(ticket) == 4 and ticket[0] == '6': \n",
    "        return \"WE\"\n",
    "    elif len(ticket) == 4:\n",
    "        return \"A5\"\n",
    "    elif len(ticket) == 5:\n",
    "        return \"CA\"\n",
    "    return \"etc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Titanic data to fill in NaN and to group categorical data\n",
    "def preprocessTitanic(dataframe):\n",
    "    # drop row w all nan\n",
    "    dataframe.dropna(how='all', inplace=True)\n",
    "\n",
    "    if \"survived\" in dataframe.columns:\n",
    "        labels = dataframe[\"survived\"].values #numpy array\n",
    "        dataframe.drop(['survived'], axis=1, inplace=True)\n",
    "    else:\n",
    "        labels = []\n",
    "\n",
    "    pclass1Age = stats.mode(dataframe[dataframe['pclass']==1]['age'])[0][0]\n",
    "    pclass2Age = np.mean(dataframe[dataframe['pclass']==2]['age'])\n",
    "    pclass3Age = stats.mode(dataframe[dataframe['pclass']==3]['age'])[0][0]\n",
    "    pclassAges = [pclass1Age, pclass2Age, pclass3Age]\n",
    "\n",
    "    pclass1Fare = np.mean(dataframe[dataframe['pclass']==1]['fare'])\n",
    "    pclass2Fare = np.mean(dataframe[dataframe['pclass']==2]['fare'])\n",
    "    pclass3Fare = stats.mode(dataframe[dataframe['pclass']==3]['fare'])[0][0]\n",
    "    pclassFares = [pclass1Fare, pclass2Fare, pclass3Fare]\n",
    "\n",
    "    pclassCabins = [\"C\", \"D\", \"E\"]\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        app = dataframe[dataframe['pclass']==i]['age'].apply(fillInWithConstant, constant=pclassAges[i-1])\n",
    "        dataframe.loc[np.array(app.index), 'age'] = app.values\n",
    "        app = dataframe[dataframe['pclass']==i]['fare'].apply(fillInWithConstant, constant=pclassFares[i-1])\n",
    "        dataframe.loc[np.array(app.index), 'fare'] = app.values\n",
    "        app = dataframe[dataframe['pclass']==i]['cabin'].apply(fillInWithConstant, constant=pclassCabins[i-1], firstLetter=True)\n",
    "        dataframe.loc[np.array(app.index), 'cabin'] = app.values\n",
    "\n",
    "    dataframe['ticket'] = dataframe['ticket'].apply(getTicketGroup)\n",
    "\n",
    "    for p in np.unique(dataframe['ticket']):\n",
    "        if p != \"FC\" and p != \"SC\":\n",
    "            cons = \"S\"\n",
    "        else:\n",
    "            cons = \"C\"\n",
    "        app = dataframe[dataframe['ticket']==p]['embarked'].apply(fillInWithConstant, constant=cons)\n",
    "        dataframe.loc[np.array(app.index), 'embarked'] = app.values\n",
    "\n",
    "    categorical_cols = ['pclass', 'sex', 'embarked', 'ticket', 'cabin']\n",
    "    numerical_cols = [col for col in dataframe.columns.values if col not in categorical_cols]\n",
    "\n",
    "    dataframe['pclass'] = dataframe['pclass'].astype(str)\n",
    "    \n",
    "    # Map categories to binary variables\n",
    "    records = dataframe[categorical_cols].to_dict(orient='records')\n",
    "    vect = DictVectorizer(sparse=False)\n",
    "    featureVects = vect.fit_transform(records)\n",
    "    colnames = vect.get_feature_names() #get name of cols\n",
    "\n",
    "    features = pd.DataFrame(dataframe[numerical_cols])\n",
    "    for i in range(len(colnames)):\n",
    "        features[colnames[i]] = featureVects[:, i]\n",
    "    \n",
    "    # Export preprocessed data \n",
    "#     features['survived'] = labels\n",
    "#     features.to_csv(\"./titanic_training_processed.csv\", index=True, index_label=\"pclass\")\n",
    "\n",
    "    return features.values, labels, features.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, splitRule, curDepth=0, label=None):\n",
    "        self.curDepth = curDepth\n",
    "        self.splitRule = splitRule\n",
    "        self.label = label #if leaf\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    def setLabel(self, label):\n",
    "        self.label = label\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, maxDepth, mPercent=1, debug=False):\n",
    "        self.root = None\n",
    "        self.maxDepth = maxDepth\n",
    "        self.numFeatures = -1\n",
    "        self.mPercent = mPercent\n",
    "        self.predictions = None\n",
    "        self.test_data = None\n",
    "        self.debug=debug\n",
    "        \n",
    "    @staticmethod\n",
    "    def printTree(node, depth):\n",
    "        if node is None:\n",
    "            return \"\"\n",
    "        if node.splitRule is not None:\n",
    "            featureIdx, thresh = node.splitRule[0], node.splitRule[1]\n",
    "            ret = \"\\t\"*depth + \"Split feature: {} ({}), Split threshold: {} \\n\".format(features[featureIdx], featureIdx, thresh)\n",
    "        else:\n",
    "            ret = \"\\t\"*depth + \"Leaf label: {} ({}) \\n\".format(class_names[node.label], node.label)\n",
    "        ret += DecisionTree.printTree(node.left, depth+1)\n",
    "        ret += DecisionTree.printTree(node.right, depth+1)\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        ret = DecisionTree.printTree(self.root, 0)\n",
    "        return ret\n",
    "    \n",
    "    # takes in labels of data stored @ node & compute entropy for the distribution of the labels (y)\n",
    "    @staticmethod\n",
    "    def entropy(labels):\n",
    "        unique_labels = np.unique(labels)\n",
    "        numLabels = len(labels)\n",
    "        label_portion = [np.count_nonzero(labels == label)/numLabels+10**(-6) for label in unique_labels]\n",
    "        return -1*sum([p*np.log(p) for p in label_portion])\n",
    "\n",
    "    #calculates information gain given a vector of features (X), labels (y) and a split threshold\n",
    "    @staticmethod\n",
    "    def information_gain(features, labels, threshold):\n",
    "        hS = DecisionTree.entropy(labels)\n",
    "        hAfter = DecisionTree.entropy_hAfter(features, labels, threshold)\n",
    "        return hS - hAfter\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy_hAfter(features, labels, threshold):\n",
    "        sL, sR = labels[np.where(features<=threshold)[0]], labels[np.where(features>threshold)[0]]\n",
    "\n",
    "        lenSL, lenSR = len(sL), len(sR)\n",
    "        hAfter = (lenSL*DecisionTree.entropy(sL) + lenSR*DecisionTree.entropy(sR))/(lenSL+lenSR)\n",
    "        return hAfter\n",
    "\n",
    "    #calculates the gini impurity given all the labels (y)\n",
    "    @staticmethod\n",
    "    def gini_impurity(labels):\n",
    "        unique_labels = np.unique(labels)\n",
    "        numLabels = len(labels)\n",
    "        label_portion = [np.count_nonzero(labels == label)/numLabels for label in unique_labels]\n",
    "        return sum([p*(1-p) for p in label_portion])\n",
    "\n",
    "    # calculates reduction in impurity gain given a vector of features and a split threshold\n",
    "    @staticmethod\n",
    "    def gini_purification(features, labels, threshold):\n",
    "        hS = DecisionTree.gini_impurity(labels)\n",
    "        hAfter = DecisionTree.gini_hAfter(features, labels, threshold)\n",
    "        return hS - hAfter\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini_hAfter(features, labels, threshold):\n",
    "        sL, sR = labels[np.where(features<=threshold)[0]], labels[np.where(features>threshold)[0]]\n",
    "        lenSL, lenSR = len(sL), len(sR)\n",
    "        hAfter = (lenSL*DecisionTree.gini_impurity(sL) + lenSR*DecisionTree.gini_impurity(sR))/(lenSL+lenSR)\n",
    "        return hAfter\n",
    "        \n",
    "    #return a split of the dataset given an index of the feature and a threshold for it\n",
    "    @staticmethod\n",
    "    def split(features, labels, idx, thresh):\n",
    "        sLData = features[np.where(features[:, idx]<=thresh)[0]]\n",
    "        sLLabels = labels[np.where(features[:, idx]<=thresh)[0]]\n",
    "        sRData = features[np.where(features[:, idx]>thresh)[0]]\n",
    "        sRLabels = labels[np.where(features[:, idx]>thresh)[0]]\n",
    "        return (sLData, sLLabels), (sRData, sRLabels)\n",
    "    \n",
    "    def segmenter(self, data, labels):\n",
    "        if len(data) < 2 or len(labels) < 2 or len(set(labels)) == 1:\n",
    "            return None\n",
    "\n",
    "        bestGini, bestInfoGain = np.inf, np.inf\n",
    "        bestFeature, bestThreshold = -1, 0\n",
    "        threshMin, threshMax = 0, 0\n",
    "        \n",
    "        possibleFeatures = self.sampleFeatures()\n",
    "        for i in possibleFeatures:\n",
    "            features = data[:, i] #ith col/feature\n",
    "            feature_mode = Counter(features).most_common(1)[0][0]\n",
    "            \n",
    "            threshR = feature_mode + .5\n",
    "            childGini = DecisionTree.gini_hAfter(features, labels, threshR)\n",
    "            if childGini < bestGini:\n",
    "                bestFeature, bestGini, bestThreshold = i, childGini, threshR\n",
    "                threshMin, threshMax = threshR, max(features)\n",
    "\n",
    "        features = data[:, bestFeature]\n",
    "        #thresholds = np.linspace(threshMin, threshMax, num=8)\n",
    "        thresholds = np.linspace(min(features), max(features), num=20)\n",
    "        for thresh in thresholds:\n",
    "            childGini = DecisionTree.gini_hAfter(features, labels, thresh)\n",
    "            \n",
    "            if childGini < bestGini:\n",
    "                bestThreshold, bestGini = thresh, childGini\n",
    "                \n",
    "        return [bestFeature, bestThreshold]\n",
    "    \n",
    "    def sampleFeatures(self):\n",
    "        if self.mPercent == 1:\n",
    "            return self.allFeatures\n",
    "        return np.random.choice(self.allFeatures, int(self.mPercent * self.numFeatures), replace=False)\n",
    "        #return np.random.choice(self.allFeatures, int(self.numFeatures**.5), replace=False)\n",
    "    \n",
    "    # fit the model to a training set.\n",
    "    def fit(self, data, labels):\n",
    "        if len(data) < 1 or len(labels) < 1:\n",
    "            return None\n",
    "\n",
    "        if self.numFeatures == -1:\n",
    "            self.numFeatures = len(data[0])\n",
    "            self.allFeatures = np.arange(self.numFeatures)\n",
    "        return self.fitHelper(data, labels, 0)\n",
    "\n",
    "    def fitHelper(self, data, labels, curDepth):\n",
    "        lenData = len(data)\n",
    "        if lenData < 1:\n",
    "            return None\n",
    "\n",
    "        stopFlag = False\n",
    "        most_common = Counter(labels).most_common(1)[0]\n",
    "        \n",
    "        if most_common[1] == lenData or most_common[1]/lenData >= 0.95:\n",
    "            if self.debug:\n",
    "                print(len(data), 'end bc have clear label') \n",
    "            stopFlag = True\n",
    "        if curDepth >= self.maxDepth or lenData < 10:\n",
    "            if self.debug:\n",
    "                print(len(data), 'end bc reached maxdepth') \n",
    "            stopFlag = True\n",
    "        \n",
    "        if not stopFlag:\n",
    "            thisNode = Node(splitRule=self.segmenter(data, labels), curDepth = curDepth)\n",
    "            \n",
    "            if thisNode.splitRule is not None:\n",
    "                    (sLData, sLLabels), (sRData, sRLabels) = DecisionTree.split(data, labels, thisNode.splitRule[0], thisNode.splitRule[1])\n",
    "                    if self.debug:\n",
    "                        print(len(data), len(sLData), len(sRData))\n",
    "                    if len(sLData) < 5 or len(sRData) < 5:\n",
    "                        if self.debug:\n",
    "                            print(len(data), 'end bc next split too small') \n",
    "                        stopFlag = True\n",
    "                    else:\n",
    "                        if self.root is None:\n",
    "                            self.root = thisNode\n",
    "                        thisNode.left = self.fitHelper(sLData, sLLabels, curDepth + 1)\n",
    "                        thisNode.right = self.fitHelper(sRData, sRLabels, curDepth + 1)\n",
    "            else:\n",
    "                if self.debug:\n",
    "                    print(len(data), 'end bc no splitRule') \n",
    "                stopFlag = True\n",
    "                \n",
    "        if stopFlag:\n",
    "            if self.debug:\n",
    "                print('turning into a leaf', len(data))\n",
    "            thisNode = Node(splitRule=None, curDepth = curDepth)\n",
    "            if self.root is None:\n",
    "                self.root = thisNode\n",
    "            thisNode.setLabel(int(most_common[0]))\n",
    "        return thisNode\n",
    "\n",
    "    #predict the labels for input data (X) \n",
    "    def predict(self, data):\n",
    "        self.predictions = np.full(len(data), -1)\n",
    "        self.test_data = data\n",
    "        self.predictHelper(self.root, np.arange(len(data)))\n",
    "        return self.predictions\n",
    "        \n",
    "    # return list of labels given data\n",
    "    def predictHelper(self, curNode, indices):\n",
    "        if curNode is None:\n",
    "            return  \n",
    "        if curNode.label is not None:\n",
    "            self.predictions[indices] = curNode.label\n",
    "        else:\n",
    "            featureIdx, thresh = curNode.splitRule[0], curNode.splitRule[1]\n",
    "            indicesL, indicesR = indices[np.where(self.test_data[indices][:, featureIdx]<=thresh)[0]], indices[np.where(self.test_data[indices][:, featureIdx]>thresh)[0]]\n",
    "            self.predictHelper(curNode.left, indicesL)\n",
    "            self.predictHelper(curNode.right, indicesR)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, numTrees=10, maxDepth=5, samplePercent=.8, mPercent=.5):\n",
    "        self.numTrees = numTrees\n",
    "        self.maxDepth = maxDepth\n",
    "        self.samplePercent = samplePercent\n",
    "        self.mPercent = mPercent\n",
    "        self.trees = None\n",
    "\n",
    "    # return indices selected randomly with replacement\n",
    "    def bagging(self, dataIndices):\n",
    "        return np.random.choice(dataIndices, int(self.samplePercent*len(dataIndices)), replace=True)\n",
    "\n",
    "    # fit the model to a training set.\n",
    "    def fit(self, X, y):\n",
    "        dataIndices = np.arange(len(X))\n",
    "        results = []\n",
    "        for i in range(self.numTrees):\n",
    "            idx = self.bagging(dataIndices)\n",
    "            data, label = X[idx], y[idx]\n",
    "            tree = DecisionTree(self.maxDepth, self.mPercent)\n",
    "            tree.fit(data, label)\n",
    "            results.append(tree)\n",
    "        self.trees = results\n",
    "        \n",
    "    #multiprocessing ver\n",
    "#     def fit(self, X, y): \n",
    "#         dataIndices = np.arange(len(X))\n",
    "#         treeBagIdx = [self.bagging(dataIndices) for i in range(self.numTrees)]\n",
    "        \n",
    "#         trees = mp.Queue()\n",
    "#         processes = [mp.Process(target=self.fitTree, args=(X, y, treeBagIdx[i], trees)) for i in range(self.numTrees)]\n",
    "#         for p in processes:\n",
    "#             p.start()\n",
    "#         for p in processes:\n",
    "#             p.join()\n",
    "#         results = [trees.get() for p in processes]\n",
    "#         self.trees = results\n",
    "#     def fitTree(self, X, y, bagIdx, trees):\n",
    "#         data, label = X[bagIdx], y[bagIdx]\n",
    "#         tree = DecisionTree(self.maxDepth, self.mPercent)\n",
    "#         tree.fit(data, label)\n",
    "#         trees.put(tree)\n",
    "    \n",
    "    # predict the labels for input data \n",
    "    def predict(self, X):\n",
    "        allPreds = [tree.predict(X) for tree in self.trees]\n",
    "        predictions = scipy.stats.mode(allPreds)[0][0]\n",
    "        return predictions, allPreds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out my code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "dataset = \"titanic\"\n",
    "#dataset = \"spam\"\n",
    "\n",
    "X, y, Z, features, class_names = loadData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7638190954773869 0.8322903629536921\n"
     ]
    }
   ],
   "source": [
    "# 2.4 DecisionTree: Training & Validation accuracies\n",
    "\n",
    "valid_data, valid_label, train_data, train_label = sample(X, y, int(0.2*len(X))) #20/80 valid/train\n",
    "\n",
    "m = 1\n",
    "maxDepth = 10\n",
    "classifier = DecisionTree(maxDepth, m)\n",
    "classifier.fit(train_data, train_label)\n",
    "valid_pred = classifier.predict(valid_data)\n",
    "valid_score = accuracy_score(valid_label, valid_pred)\n",
    "train_pred = classifier.predict(train_data)\n",
    "train_score = accuracy_score(train_label, train_pred)\n",
    "print(valid_score, train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809 0.827\n"
     ]
    }
   ],
   "source": [
    "# 2.4 RandomForest: Training & Validation accuracies\n",
    "numTrees = 10\n",
    "maxDepth = 10\n",
    "samplePercent = 0.9\n",
    "mPercent = 1\n",
    "forest = RandomForest(numTrees, maxDepth, samplePercent, mPercent)\n",
    "forest.fit(train_data, train_label)\n",
    "valid_pred, _ = forest.predict(valid_data)\n",
    "valid_score = round(accuracy_score(valid_label, valid_pred), 3)\n",
    "train_pred, _ = forest.predict(train_data)\n",
    "train_score = round(accuracy_score(train_label, train_pred), 3)\n",
    "print(valid_score, train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7678916827852998\n",
      "2 0.7843326885880078\n",
      "3 0.7978723404255319\n",
      "4 0.8007736943907157\n",
      "5 0.8056092843326886\n",
      "6 0.8152804642166345\n",
      "7 0.8172147001934236\n",
      "8 0.8172147001934236\n",
      "9 0.8230174081237911\n",
      "10 0.8230174081237911\n",
      "11 0.8249516441005803\n",
      "12 0.8249516441005803\n",
      "13 0.8249516441005803\n",
      "14 0.8191489361702128\n",
      "15 0.8191489361702128\n",
      "16 0.8191489361702128\n",
      "17 0.8191489361702128\n",
      "18 0.8191489361702128\n",
      "19 0.8191489361702128\n",
      "20 0.8191489361702128\n",
      "21 0.8191489361702128\n",
      "22 0.8191489361702128\n",
      "23 0.8191489361702128\n",
      "24 0.8191489361702128\n",
      "25 0.8191489361702128\n",
      "26 0.8191489361702128\n",
      "27 0.8191489361702128\n",
      "28 0.8191489361702128\n",
      "29 0.8191489361702128\n",
      "30 0.8191489361702128\n",
      "31 0.8191489361702128\n",
      "32 0.8191489361702128\n",
      "33 0.8191489361702128\n",
      "34 0.8191489361702128\n",
      "35 0.8191489361702128\n",
      "36 0.8191489361702128\n",
      "37 0.8191489361702128\n",
      "38 0.8191489361702128\n",
      "39 0.8191489361702128\n",
      "40 0.8191489361702128\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.5.3 DecisionTree on Spam: Validation Accuracy with Varying Max Depth\n",
    "\n",
    "valid_data, valid_label, train_data, train_label = sample(X, y, int(0.2*len(X))) #20/80 valid/train\n",
    "\n",
    "m = 1\n",
    "depths = np.arange(1, 41)\n",
    "validation_accuracies = []\n",
    "for depth in depths:\n",
    "    maxDepth = depth\n",
    "    classifier = DecisionTree(maxDepth, m)\n",
    "    classifier.fit(train_data, train_label)\n",
    "    valid_pred = classifier.predict(valid_data)\n",
    "    score = accuracy_score(valid_label, valid_pred)\n",
    "    print(depth, score)\n",
    "    validation_accuracies.append(score)\n",
    "    \n",
    "#     skTree = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "#     skTree.fit(train_data, train_label)\n",
    "#     skvalid_pred = skTree.predict(valid_data)\n",
    "#     score = accuracy_score(valid_label, skvalid_pred)\n",
    "#     print(score)\n",
    "#     dot_data = StringIO()\n",
    "#     export_graphviz(skTree, out_file=dot_data,  \n",
    "#                     filled=True, rounded=True,\n",
    "#                     special_characters=True)\n",
    "#     graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#     Image(graph.create_png())\n",
    "#     graph.write_pdf(\"tree\"+str(depth)+\".pdf\")\n",
    "\n",
    "plt.plot(depths, validation_accuracies, color='blue', label='Validation Accuracy')\n",
    "plt.xticks()\n",
    "plt.xlabel(\"Max Depth of Tree\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy with Varying Max Depth\")\n",
    "plt.legend()\n",
    "plt.savefig(\"VaryingDepth.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split feature: sex=female (19), Split threshold: 0.5 \n",
      "\tSplit feature: pclass=1.0 (16), Split threshold: 0.5 \n",
      "\t\tSplit feature: parch (3), Split threshold: 0.5 \n",
      "\t\t\tLeaf label: Died (0) \n",
      "\t\t\tLeaf label: Died (0) \n",
      "\t\tLeaf label: Died (0) \n",
      "\tSplit feature: pclass=3.0 (18), Split threshold: 0.5 \n",
      "\t\tSplit feature: fare (4), Split threshold: 26.5 \n",
      "\t\t\tLeaf label: Survived (1) \n",
      "\t\t\tLeaf label: Survived (1) \n",
      "\t\tSplit feature: ticket=A5 (25), Split threshold: 0.5 \n",
      "\t\t\tLeaf label: Survived (1) \n",
      "\t\t\tLeaf label: Died (0) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.6\n",
    "maxDepth = 3\n",
    "m=1\n",
    "classifier = DecisionTree(maxDepth, m)\n",
    "classifier.fit(X, y)\n",
    "print(classifier)\n",
    "# skTree = tree.DecisionTreeClassifier(max_depth=maxDepth)\n",
    "# skTree.fit(X, y)\n",
    "# dot_data = StringIO()\n",
    "# export_graphviz(skTree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())\n",
    "# graph.write_pdf(\"tree\"+str(depth)+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.811 0.817\n",
      "0.2 0.835 0.823\n",
      "0.30000000000000004 0.836 0.826\n",
      "0.4 0.828 0.829\n",
      "0.5 0.832 0.832\n",
      "0.6 0.833 0.83\n",
      "0.7000000000000001 0.838 0.833\n",
      "0.8 0.831 0.83\n",
      "0.9 0.834 0.827\n",
      "1.0 0.831 0.83\n"
     ]
    }
   ],
   "source": [
    "# Kaggle: hyperparameter tuning\n",
    "valid_data, valid_label, train_data, train_label = sample(X, y, int(0.2*len(X))) #20/80 valid/train\n",
    "\n",
    "nTrees = np.arange(1, 21)#np.arange(3, 103, 10)\n",
    "percents = np.linspace(0.1,1, 10)\n",
    "\n",
    "# for d in np.arange(1, 20):\n",
    "for p in percents:\n",
    "# for n in nTrees:\n",
    "    numTrees = 10\n",
    "    maxDepth = 17\n",
    "    samplePercent = .8\n",
    "    mPercent = p\n",
    "    forest = RandomForest(numTrees, maxDepth, samplePercent, mPercent)\n",
    "    forest.fit(train_data, train_label)\n",
    "    valid_pred, allPreds = forest.predict(valid_data)\n",
    "    vscore = round(accuracy_score(valid_label, valid_pred), 3)\n",
    "    train_pred, _ = forest.predict(train_data)\n",
    "    tscore = round(accuracy_score(train_label, train_pred), 3)\n",
    "    print(p, vscore, tscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
